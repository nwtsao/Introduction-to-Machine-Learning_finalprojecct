# -*- coding: utf-8 -*-
"""Final Project_109705012_111705066

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ySTYxfyzIxcciKr4NuylNyt-LvJve4sj
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
import numpy as np
from sklearn.metrics import accuracy_score
from google.colab import drive
drive.mount('/content/drive')

# data transform
transform = transforms.Compose([
    transforms.Resize((64, 64)), # 壓縮成64*64像素
    transforms.ToTensor(), # 將圖像轉換為張量格式
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) #標準化圖像
])

# Data by Google Drive
train_data_path = '/content/drive/MyDrive/dataset/train'
test_data_path = '/content/drive/MyDrive/dataset/dataset_demo' # demo testing data
#test_data_path = '/content/drive/MyDrive/dataset/test'

# load data
train_data = datasets.ImageFolder(root=train_data_path, transform=transform)
train_loader = DataLoader(train_data, batch_size=32, shuffle=True)

test_data = datasets.ImageFolder(root=test_data_path, transform=transform)
test_loader = DataLoader(test_data, batch_size=32, shuffle=False)

#DCNN模型。包括兩個卷積層、兩個個池化層及一個輸出層。使用ReLU函數來防止overfitting
class DCNNModel(nn.Module):
    def __init__(self):
        super(DCNNModel, self).__init__() # 調用nn.Module的構造函數
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv3 = nn.Conv2d(32, 1, kernel_size=3, padding=1)
        self.pool3 = nn.AdaptiveAvgPool2d(1) # average pooling

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = self.pool1(x)
        x = torch.relu(self.conv2(x))
        x = self.pool2(x)
        x = torch.sigmoid(self.conv3(x)) # binary
        x = self.pool3(x)
        return x

model = DCNNModel()

# convolution layer : get feature of image
# pooling layer : decrease size of feature image, keep main feature
# output layer : fully connected layer using sigmoid (binary problem,adult/children)

# loss function and optimizer
criterion = nn.BCELoss() # binary cross entrophy
optimizer = optim.Adam(model.parameters(), lr=0.01)

loss_values = []
# train model
num_epochs = 30
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for inputs, labels in train_loader:
        labels = labels.float().view(-1, 1, 1, 1)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')

"""Accuracy"""

def evaluate_model(model, dataloader):
    model.eval()
    all_labels = []
    all_preds = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            labels = labels.float().view(-1, 1, 1, 1)
            outputs = model(inputs)
            preds = outputs > 0.5
            all_labels.extend(labels.numpy().flatten())
            all_preds.extend(preds.numpy().flatten())

    accuracy = accuracy_score(all_labels, all_preds)
    return accuracy

train_accuracy = evaluate_model(model, train_loader)
test_accuracy = evaluate_model(model, test_loader)

#print(f'Train Accuracy: {train_accuracy:.4f}')
print(f'Test Accuracy: {test_accuracy:.4f}')

!pip install thop
from thop import profile

# flop and number of parameter
input_sample = torch.randn(1, 3, 64, 64)
flops, params = profile(model, inputs=(input_sample, ))

print(f'Total Parameters: {params}')
print(f'Total FLOPs: {flops}')